{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YScUrgcmlb3"
      },
      "source": [
        "# Masoscience (advance LSTM model)\n",
        "\n",
        "**Author:** Mir Yasin Zeinaliyan\n",
        "\n",
        "**Email:** yasinprodebian@gmail.com  \n",
        "\n",
        "**Github:** https://github.com/yasin-pro/masoscience\n",
        "\n",
        "**Description:** In this project, we implement an advanced LSTM model, which is provided for free, but there are no other models in the demo, but we implemented this part completely, and this part is enough to implement the other processes of the project. Unlike this section, although it is a demo, we have presented a very good model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF0u0Ktj7v6H"
      },
      "source": [
        "### Install the necessary tools\n",
        "\n",
        "To run the codes of this project, you must install the relevant tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZvownbB3Qd5"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT6AqCss_Fw0"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "In this section, I entered the code of all the libraries that are required to run the following codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-G2zpo8L_F9o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from kerastuner import HyperModel\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, TimeDistributed, RepeatVector, BatchNormalization, LeakyReLU, Attention, Add\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlD_yuZT-4P6"
      },
      "source": [
        "### Read data\n",
        "\n",
        "In this section, we read the prepared data and check it\n",
        "\n",
        "My data is in my Google Drive, if your data is in another path, you need to change the data reading code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmA_O5Wh-vVg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Masoscience/processed_eurusd.csv\")\n",
        "\n",
        "# df = pd.read_csv(\"processed_eurusd.csv\")\n",
        "\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJnL9egBAQvg"
      },
      "source": [
        "### Checking and reviewing data\n",
        "\n",
        "In this section, we get an overview of the data and check that we have not forgotten anything in the preparation and that the data is ready for learning and performing operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acR0jBCSAJOM"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data augmentation\n",
        "\n",
        "Data augmentation involves creating new data points from existing data by applying various transformations. By introducing slight modifications to the original data, we generate multiple versions, enabling the machine learning model to train on a more extensive and varied dataset."
      ],
      "metadata": {
        "id": "ioXOBGhxx_iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_augmentation(data, augmentation_factor=2):\n",
        "    augmented_data = data.copy()\n",
        "    original_data_length = len(data)\n",
        "\n",
        "    for _ in range(augmentation_factor):\n",
        "        new_data = data.copy()\n",
        "\n",
        "        non_cyclical_columns = [col for col in data.columns if not any(x in col for x in ['sin', 'cos'])]\n",
        "        for column in non_cyclical_columns:\n",
        "            noise = np.random.normal(0, 0.01, len(data))\n",
        "            new_data[column] = new_data[column] + noise\n",
        "\n",
        "        augmented_data = pd.concat([augmented_data, new_data])\n",
        "\n",
        "    return augmented_data.reset_index(drop=True)\n",
        "\n",
        "n_augmentations = 2\n",
        "\n",
        "augmented_data = data_augmentation(df, n_augmentations)"
      ],
      "metadata": {
        "id": "IR1ivt8y4U68"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cyclical perturbation\n",
        "  this technique involves making small, periodic changes to data that has inherent cyclical features, such as sine and cosine components. The objective of cyclical perturbation is to maintain the periodic cycles while introducing slight variations to increase the diversity of the training dataset."
      ],
      "metadata": {
        "id": "xlMnpq8P3s4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cyclical_perturbation(data, sin_columns, cos_columns, perturb_factor=0.005):\n",
        "    perturbed_data = data.copy()\n",
        "\n",
        "    for sin_col, cos_col in zip(sin_columns, cos_columns):\n",
        "        angle = np.random.normal(0, perturb_factor, len(data))\n",
        "        perturbed_data[sin_col] = perturbed_data[sin_col] * np.cos(angle) - perturbed_data[cos_col] * np.sin(angle)\n",
        "        perturbed_data[cos_col] = perturbed_data[sin_col] * np.sin(angle) + perturbed_data[cos_col] * np.cos(angle)\n",
        "\n",
        "    return perturbed_data\n",
        "\n",
        "\n",
        "cyclical_columns = ['hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos', 'week_of_month_sin', 'week_of_month_cos']\n",
        "sin_columns = [col for col in cyclical_columns if 'sin' in col]\n",
        "cos_columns = [col for col in cyclical_columns if 'cos' in col]\n",
        "\n",
        "perturb_factor = 0.005\n",
        "\n",
        "perturbed_data = cyclical_perturbation(augmented_data, sin_columns, cos_columns, perturb_factor)"
      ],
      "metadata": {
        "id": "zj_WaG8k3tSS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KMmVFQAAlxt"
      },
      "source": [
        "### Normalize the data\n",
        "\n",
        "`RobustScaler` is one of the scalers available in the scikit-learn library, used for scaling features of the data. It is particularly robust in the presence of outliers.\n",
        "\n",
        "#### How It Works\n",
        "\n",
        "Unlike `StandardScaler`, which uses the mean and standard deviation, `RobustScaler` uses the median and interquartile range (IQR) for scaling, reducing the influence of outliers on the data.\n",
        "\n",
        "The formula used by `RobustScaler` to scale each feature is as follows:\n",
        "\n",
        "---\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\text{Median}(X)}{\\text{IQR}(X)}\n",
        "$$\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "grPJGZi1sytk"
      },
      "outputs": [],
      "source": [
        "X = df.drop([\"open\", \"high\", \"low\", \"close\", \"change_percent\"], axis=1)\n",
        "y = df[[\"open\", \"high\", \"low\", \"close\", \"change_percent\"]]\n",
        "\n",
        "scaler_X = RobustScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "\n",
        "scaler_y = RobustScaler()\n",
        "y_scaled = scaler_y.fit_transform(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmQWtOOgNFe7"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "In this section, we divided the data into training and testing sections and transformed it to coordinate with the LSTM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RpakX0nFNGRF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train_base, y_test_base = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbxj5gmlhAJP"
      },
      "source": [
        "### Model prediction settings\n",
        "\n",
        "To predict the number of future steps, which refers to predicting the number of future candles, we set the number of outputs and check and adjust the number of outputs for coordination and assurance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2-4MujQthAeJ"
      },
      "outputs": [],
      "source": [
        "prediction_steps = 10\n",
        "\n",
        "y_train = np.array([y_train_base[i:i + prediction_steps] for i in range(0, len(y_train_base) - prediction_steps)])\n",
        "y_test = np.array([y_test_base[i:i + prediction_steps] for i in range(0, len(y_test_base) - prediction_steps)])\n",
        "\n",
        "X_train = X_train[:len(y_train)]\n",
        "X_test = X_test[:len(y_test)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arsUAiufKljg"
      },
      "source": [
        "### LSTM model creation\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies. They were introduced to mitigate the vanishing gradient problem in traditional RNNs, making them more effective for time series prediction, natural language processing, and other sequential tasks.\n",
        "\n",
        "This code section builds and trains an advanced LSTM model for time series forecasting, using multiple techniques such as normalization, elimination, regularization, and compression to avoid overfitting and improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Am1H87jKs1XS"
      },
      "outputs": [],
      "source": [
        "def attention_3d_block(inputs):\n",
        "    input_dim = int(inputs.shape[2])\n",
        "    a = Dense(input_dim, activation='softmax')(inputs)\n",
        "    output_attention_mul = tf.keras.layers.multiply([inputs, a])\n",
        "    return output_attention_mul\n",
        "\n",
        "class LSTMHyperModel(HyperModel):\n",
        "\n",
        "    def build(self, hp):\n",
        "        inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "        x = LSTM(units=hp.Int('units_1', min_value=32, max_value=128, step=32),\n",
        "                 return_sequences=True, kernel_regularizer=tf.keras.regularizers.L2(0.01))(inputs)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1))(x)\n",
        "\n",
        "        x = LSTM(units=hp.Int('units_2', min_value=32, max_value=128, step=32),\n",
        "                 return_sequences=True, kernel_regularizer=tf.keras.regularizers.L2(0.01))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1))(x)\n",
        "\n",
        "        x = attention_3d_block(x)\n",
        "\n",
        "        x = LSTM(units=hp.Int('units_3', min_value=32, max_value=128, step=32),\n",
        "                 return_sequences=False, kernel_regularizer=tf.keras.regularizers.L2(0.01))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = Dropout(hp.Float('dropout_3', min_value=0.2, max_value=0.5, step=0.1))(x)\n",
        "\n",
        "        x = RepeatVector(prediction_steps)(x)\n",
        "\n",
        "        x = LSTM(units=hp.Int('units_4', min_value=32, max_value=128, step=32),\n",
        "                 return_sequences=True, kernel_regularizer=tf.keras.regularizers.L2(0.01))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = Dropout(hp.Float('dropout_4', min_value=0.2, max_value=0.5, step=0.1))(x)\n",
        "\n",
        "        x = LSTM(units=hp.Int('units_5', min_value=32, max_value=128, step=32),\n",
        "                 return_sequences=True, kernel_regularizer=tf.keras.regularizers.L2(0.01))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = Dropout(hp.Float('dropout_5', min_value=0.2, max_value=0.5, step=0.1))(x)\n",
        "\n",
        "        x = TimeDistributed(Dense(5, kernel_regularizer=tf.keras.regularizers.L2(0.01)))(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=x)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
        "            loss='mean_squared_error')\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdQ1Lwgx0HWz"
      },
      "source": [
        "### models Callbacks\n",
        "\n",
        "A few callbacks have been added to make it a good model, especially for storage and significant improvements and..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZlYl9jdU0LL8"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "checkpoint = ModelCheckpoint('lstm_model_checkpoint.keras', monitor='val_loss', save_best_only=True, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMOoJ5uiHP4o"
      },
      "source": [
        "### Train model\n",
        "\n",
        "In this part, the most important part of the program is for the model to learn to use it for prediction\n",
        "\n",
        "The implemented model is of HyperModel type, so we can find its best settings with existing techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvLJM5WPHSHI"
      },
      "outputs": [],
      "source": [
        "tuner = RandomSearch(\n",
        "    LSTMHyperModel(),\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='/content/drive/My Drive/Masoscience/lstm/lstm_hyperparameter_tuning',\n",
        "    project_name='/content/drive/My Drive/Masoscience/lstm/lstm_stock_prediction')\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=50,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[early_stopping, reduce_lr, checkpoint],\n",
        "             verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLexhHdp3xIs"
      },
      "source": [
        "### Get best model\n",
        "\n",
        "We get the best model with the following line of code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1_NeR5e3wxq"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDqBQl2N0yGy"
      },
      "source": [
        "### Save model\n",
        "\n",
        "We save the seen training model at the end so that we don't run the operation on it every time to use it for prediction. As it turns out, this process is very time-consuming.\n",
        "\n",
        "I saved it in my Google Drive. To save it in another address, you need to change the following code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHWBHhC60yUQ"
      },
      "outputs": [],
      "source": [
        "# best_model.save(\"models/lstm_model.h5\")\n",
        "best_model.save(\"/content/drive/My Drive/Masoscience/lstm/lstm_model.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RF0u0Ktj7v6H",
        "JT6AqCss_Fw0",
        "BlD_yuZT-4P6",
        "gJnL9egBAQvg",
        "ioXOBGhxx_iN",
        "xlMnpq8P3s4v",
        "4KMmVFQAAlxt",
        "cmQWtOOgNFe7",
        "cbxj5gmlhAJP",
        "arsUAiufKljg",
        "EdQ1Lwgx0HWz",
        "GMOoJ5uiHP4o",
        "LLexhHdp3xIs",
        "FDqBQl2N0yGy"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}